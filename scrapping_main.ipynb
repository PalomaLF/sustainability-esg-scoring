{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import selenium\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver   # driver de selenium\n",
    "from selenium.webdriver.chrome.options import Options   # opciones del driver\n",
    "from selenium.webdriver.common.by import By # By es para buscar por tag, clase, id...\n",
    "from selenium.webdriver.common.keys import Keys  # para usar teclado\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting scrapping process getting all companies and scores available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "serv = Service(passwords.driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opciones=Options()\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.add_argument('user-data-dir=cookies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening google chrome\n",
    "driver = webdriver.Chrome(service = serv, options = opciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing website\n",
    "driver.get('https://www.sustainalytics.com/esg-ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page(range_start,range_end,jump):\n",
    "    paginas = [str(x) for x in list(range(range_start,range_end,jump))]\n",
    "    for x in paginas:\n",
    "        driver.find_element(By.LINK_TEXT, x).click()\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_current_page():\n",
    "    table = driver.find_element(By.ID, 'company_ratings')\n",
    "    html = table.get_attribute('outerHTML')\n",
    "    soup = bs(html, 'html.parser')\n",
    "    #extracting lists of the different elements\n",
    "    companies.extend([result.get_text() for result in (soup.find_all('a')[:10])])\n",
    "    symbols.extend([result.get_text() for result in (soup.find_all('small'))])\n",
    "    ratings.extend([result.get_text() for result in (soup.find_all('div', class_='col-2'))])\n",
    "    risk.extend([result.get_text() for result in (soup.find_all('div', class_=\"col-lg-6 col-md-10\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we extract the first page:\n",
    "#selecting table and creating soup\n",
    "table = driver.find_element(By.ID, 'company_ratings')\n",
    "html = table.get_attribute('outerHTML')\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting lists of the different elements\n",
    "companies = []\n",
    "symbols = []\n",
    "ratings = []\n",
    "risk = []\n",
    "\n",
    "companies.extend([result.get_text() for result in (soup.find_all('a')[:10])])\n",
    "symbols.extend([result.get_text() for result in (soup.find_all('small'))])\n",
    "ratings.extend([result.get_text() for result in (soup.find_all('div', class_='col-2'))])\n",
    "risk.extend([result.get_text() for result in (soup.find_all('div', class_=\"col-lg-6 col-md-10\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = df['companies'].to_list()\n",
    "symbols = df['symbols'].to_list()\n",
    "ratings = df['ratings'].to_list()\n",
    "risk = df['risk'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting the rest of the pages\n",
    "paginas = [str(x) for x in list(range(2,1381))]\n",
    "len(paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 s, sys: 199 ms, total: 2.54 s\n",
      "Wall time: 25min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in paginas:\n",
    "    driver.find_element(By.LINK_TEXT, x).click()       \n",
    "    #selecting table and creating soup\n",
    "    table = driver.find_element(By.ID, 'company_ratings')\n",
    "    html = table.get_attribute('outerHTML')\n",
    "    soup = bs(html, 'html.parser')\n",
    "    #extracting lists of the different elements\n",
    "    companies.extend([result.get_text() for result in (soup.find_all('a')[:10])])\n",
    "    symbols.extend([result.get_text() for result in (soup.find_all('small'))])\n",
    "    ratings.extend([result.get_text() for result in (soup.find_all('div', class_='col-2'))])\n",
    "    risk.extend([result.get_text() for result in (soup.find_all('div', class_=\"col-lg-6 col-md-10\"))])\n",
    "    time.sleep(3) #allowing some delay so that the site can charge the page and the driver can find the next page button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14371, 14371, 14371, 14371)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(companies),len(symbols),len(ratings),len(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page(700,245,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_current_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame with multiple columns to the single column Companies",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ps/6ygxcm2s5xj2_hmh0rwz7d640000gn/T/ipykernel_78018/940232336.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Companies'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Ironhack/proyecto_final/.conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4285\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4286\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4287\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4288\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4290\u001b[0m         elif (\n\u001b[1;32m   4291\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4292\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ironhack/proyecto_final/.conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4444\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4447\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   4448\u001b[0m                 \u001b[0;34m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4449\u001b[0m                 \u001b[0;34mf\"column {key}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4450\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set a DataFrame with multiple columns to the single column Companies"
     ]
    }
   ],
   "source": [
    "#since we get some errors and lose some information, we are extracting the information twice, merging and eliminating duplicates\n",
    "df= pd.DataFrame()\n",
    "df['Companies'] = companies\n",
    "df['Symbols'] = symbols\n",
    "df['Ratings'] = ratings\n",
    "df['Risk'] = risk\n",
    "print(len(df[df.duplicated(subset='companies') == True]))\n",
    "df.drop_duplicates(keep = 'first', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg['Companies'] = companies\n",
    "esg['Symbols'] = symbols\n",
    "esg['Ratings'] = ratings\n",
    "esg['Risk'] = risk\n",
    "print(len(esg[esg.duplicated(subset='Companies') == True]))\n",
    "esg.drop_duplicates(subset='Companies', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_esg = pd.concat([esg,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13791"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_esg = companies_esg.drop_duplicates(subset =  'Companies')\n",
    "len(companies_esg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#companies_esg.to_csv('companies_esg.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapping rest of data using company names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating companies with and without Identifier\n",
    "names = (companies_esg['Companies']).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quitando empresas que ya tenemos QUITAR DEL FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = pd.read_csv('data/final tables sql/companies.csv')\n",
    "filter = filter['Companies'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [x for x in names if x not in filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hasta aquÃ­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11303"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_r = names[873:]\n",
    "shuffle(names_r)\n",
    "len(names_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_symb ={' Co.':'', ' Ltd.':'',' Inc.':'', ' Plc':'', ' Corp.':'', ' CO. LTD.':'', ' Corp.':'', ',':''}\n",
    "for k,v in replace_symb.items():\n",
    "    names_r = [w.replace(k,v) for w in names_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "opciones.page_load_strategy = 'none'\n",
    "opciones.add_argument('--disk-cache-dir=/path/to/cache')\n",
    "driver = webdriver.Chrome(service = serv, options = opciones)\n",
    "wait = WebDriverWait(driver, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creating empty lists to save scrapes and not lose them if scrapping process fails:\n",
    "industry = []\n",
    "country = []\n",
    "symbol = []\n",
    "company_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping industry and country from the same website, we start the scrapping indicating the first website to start:\n",
    "driver.get('https://www.sustainalytics.com/esg-rating/shinko-shoji-co-ltd/1008754046')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping current website:\n",
    "table = driver.find_element(By.CLASS_NAME, 'col-md-8.col-sm-12.company-data')\n",
    "html = table.get_attribute('outerHTML')\n",
    "soup = bs(html, 'html.parser')\n",
    "company_name.append((soup.find('h2')).get_text())\n",
    "industry.append((soup.find('strong', class_='industry-group')).get_text())\n",
    "country.append((soup.find('strong', class_='country')).get_text())\n",
    "symbol.append((soup.find('strong', class_='identifier')).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x0000000100e172f8 chromedriver + 4625144\n1   chromedriver                        0x0000000100e0eea3 chromedriver + 4591267\n2   chromedriver                        0x0000000100a0de6a chromedriver + 392810\n3   chromedriver                        0x0000000100a5941d chromedriver + 701469\n4   chromedriver                        0x0000000100a595b1 chromedriver + 701873\n5   chromedriver                        0x0000000100a9d1c4 chromedriver + 979396\n6   chromedriver                        0x0000000100a7b89d chromedriver + 841885\n7   chromedriver                        0x0000000100a9a68f chromedriver + 968335\n8   chromedriver                        0x0000000100a7b613 chromedriver + 841235\n9   chromedriver                        0x0000000100a4c3da chromedriver + 648154\n10  chromedriver                        0x0000000100a4cd1e chromedriver + 650526\n11  chromedriver                        0x0000000100dd7ac0 chromedriver + 4364992\n12  chromedriver                        0x0000000100ddce86 chromedriver + 4386438\n13  chromedriver                        0x0000000100dbc72e chromedriver + 4253486\n14  chromedriver                        0x0000000100dddbc9 chromedriver + 4389833\n15  chromedriver                        0x0000000100daea79 chromedriver + 4196985\n16  chromedriver                        0x0000000100dfdb78 chromedriver + 4520824\n17  chromedriver                        0x0000000100dfdd57 chromedriver + 4521303\n18  chromedriver                        0x0000000100e0eae3 chromedriver + 4590307\n19  libsystem_pthread.dylib             0x00007ff8184e9202 _pthread_start + 99\n20  libsystem_pthread.dylib             0x00007ff8184e4bab thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 10\u001b[0m\n\u001b[1;32m      9\u001b[0m     element \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch-link.js-fix-path\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m---> 10\u001b[0m     element\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#if website takes too long to charge or there is any error:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:94\u001b[0m, in \u001b[0;36mWebElement.click\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Clicks the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(Command\u001b[38;5;241m.\u001b[39mCLICK_ELEMENT)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[1;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: stale element not found\n  (Session info: chrome=122.0.6261.112); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n0   chromedriver                        0x0000000100e172f8 chromedriver + 4625144\n1   chromedriver                        0x0000000100e0eea3 chromedriver + 4591267\n2   chromedriver                        0x0000000100a0de6a chromedriver + 392810\n3   chromedriver                        0x0000000100a13394 chromedriver + 414612\n4   chromedriver                        0x0000000100a1512b chromedriver + 422187\n5   chromedriver                        0x0000000100a151cc chromedriver + 422348\n6   chromedriver                        0x0000000100a5b0b2 chromedriver + 708786\n7   chromedriver                        0x0000000100a4e2c1 chromedriver + 656065\n8   chromedriver                        0x0000000100a7b872 chromedriver + 841842\n9   chromedriver                        0x0000000100a4ddb8 chromedriver + 654776\n10  chromedriver                        0x0000000100a7ba2e chromedriver + 842286\n11  chromedriver                        0x0000000100a9a68f chromedriver + 968335\n12  chromedriver                        0x0000000100a7b613 chromedriver + 841235\n13  chromedriver                        0x0000000100a4c3da chromedriver + 648154\n14  chromedriver                        0x0000000100a4cd1e chromedriver + 650526\n15  chromedriver                        0x0000000100dd7ac0 chromedriver + 4364992\n16  chromedriver                        0x0000000100ddce86 chromedriver + 4386438\n17  chromedriver                        0x0000000100dbc72e chromedriver + 4253486\n18  chromedriver                        0x0000000100dddbc9 chromedriver + 4389833\n19  chromedriver                        0x0000000100daea79 chromedriver + 4196985\n20  chromedriver                        0x0000000100dfdb78 chromedriver + 4520824\n21  chromedriver                        0x0000000100dfdd57 chromedriver + 4521303\n22  chromedriver                        0x0000000100e0eae3 chromedriver + 4590307\n23  libsystem_pthread.dylib             0x00007ff8184e9202 _pthread_start + 99\n24  libsystem_pthread.dylib             0x00007ff8184e4bab thread_start + 15\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     textarea\u001b[38;5;241m.\u001b[39msend_keys(s)\n\u001b[1;32m     18\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     element \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch-link.js-fix-path\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     20\u001b[0m     element\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#stop charging website once we have the information we need:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x0000000100e172f8 chromedriver + 4625144\n1   chromedriver                        0x0000000100e0eea3 chromedriver + 4591267\n2   chromedriver                        0x0000000100a0de6a chromedriver + 392810\n3   chromedriver                        0x0000000100a5941d chromedriver + 701469\n4   chromedriver                        0x0000000100a595b1 chromedriver + 701873\n5   chromedriver                        0x0000000100a9d1c4 chromedriver + 979396\n6   chromedriver                        0x0000000100a7b89d chromedriver + 841885\n7   chromedriver                        0x0000000100a9a68f chromedriver + 968335\n8   chromedriver                        0x0000000100a7b613 chromedriver + 841235\n9   chromedriver                        0x0000000100a4c3da chromedriver + 648154\n10  chromedriver                        0x0000000100a4cd1e chromedriver + 650526\n11  chromedriver                        0x0000000100dd7ac0 chromedriver + 4364992\n12  chromedriver                        0x0000000100ddce86 chromedriver + 4386438\n13  chromedriver                        0x0000000100dbc72e chromedriver + 4253486\n14  chromedriver                        0x0000000100dddbc9 chromedriver + 4389833\n15  chromedriver                        0x0000000100daea79 chromedriver + 4196985\n16  chromedriver                        0x0000000100dfdb78 chromedriver + 4520824\n17  chromedriver                        0x0000000100dfdd57 chromedriver + 4521303\n18  chromedriver                        0x0000000100e0eae3 chromedriver + 4590307\n19  libsystem_pthread.dylib             0x00007ff8184e9202 _pthread_start + 99\n20  libsystem_pthread.dylib             0x00007ff8184e4bab thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "for s in names_r[255:2000]:     #remember changing first row index for the NEXT website to be scrapped, not the current one\n",
    "    \n",
    "    #changing url to next company\n",
    "    #if everything goes well:\n",
    "    try: \n",
    "        textarea = driver.find_element(By.ID, 'searchInput')\n",
    "        textarea.send_keys(s)\n",
    "        time.sleep(3)\n",
    "        element = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'search-link.js-fix-path')))\n",
    "        element.click()\n",
    "    #if website takes too long to charge or there is any error:\n",
    "    except:\n",
    "        driver.refresh()\n",
    "        time.sleep(10)\n",
    "        textarea = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.ID, 'searchInput')))\n",
    "        driver.execute_script(\"window.stop();\")\n",
    "        textarea.send_keys(s)\n",
    "        time.sleep(3)\n",
    "        element = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'search-link.js-fix-path')))\n",
    "        element.click()\n",
    "    #stop charging website once we have the information we need:\n",
    "    time.sleep(10)\n",
    "    element = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.ID, 'searchInput')))\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "    #scrapping elemnts and adding them to lists\n",
    "    \n",
    "    table = driver.find_element(By.CLASS_NAME, 'col-md-8.col-sm-12.company-data')\n",
    "    html = table.get_attribute('outerHTML')\n",
    "    soup = bs(html, 'html.parser')\n",
    "    company_name.append((soup.find('h2')).get_text())\n",
    "    industry.append((soup.find('strong', class_='industry-group')).get_text())\n",
    "    country.append((soup.find('strong', class_='country')).get_text())\n",
    "    symbol.append((soup.find('strong', class_='identifier')).get_text())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1872"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Companies</th>\n",
       "      <th>Symbols</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>NOMURA Co., Ltd.</td>\n",
       "      <td>TKS:9716</td>\n",
       "      <td>25.1</td>\n",
       "      <td>Medium ESG Risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Companies   Symbols  Ratings             Risk\n",
       "8049  NOMURA Co., Ltd.  TKS:9716     25.1  Medium ESG Risk"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#last company scrapped:\n",
    "companies_esg[companies_esg['Companies'] == (company_name[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next company:\n",
    "names_r.index(\"NOMURA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.DataFrame()\n",
    "backup['Industry']=industry\n",
    "backup['Country']=country\n",
    "backup['Symbols']=symbol\n",
    "backup['Companies']=company_name\n",
    "backup.to_csv('data/working data/backup_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recuperar backup\n",
    "backup = pd.read_csv('data/working data/backup_3.csv')\n",
    "industry = backup['Industry'].to_list()\n",
    "country = backup['Country'].to_list()\n",
    "symbol = backup['Symbols'].to_list()\n",
    "company_name = backup['Companies'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
